

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/source/img/siteicon.jpg">
  <link rel="icon" href="/source/img/siteicon.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Lee">
  <meta name="keywords" content="">
  
    <meta name="description" content="PATTERN RECOGNITION AND MACHINE LEARNING SYSTEMSDay 1Day 22.1 Neural Network Models and Designs2.1.1 径向基函数 Radial Basis Function Networks（1）Architecture 输入层与隐藏层完全连接  隐藏层与输出层完全连接   node  输入层的每个节点：接受输入变">
<meta property="og:type" content="article">
<meta property="og:title" content="PRMLS">
<meta property="og:url" content="https://jiajun-lab.github.io/2025/09/16/PRMLS/index.html">
<meta property="og:site_name" content="Lee&#39;s Blog">
<meta property="og:description" content="PATTERN RECOGNITION AND MACHINE LEARNING SYSTEMSDay 1Day 22.1 Neural Network Models and Designs2.1.1 径向基函数 Radial Basis Function Networks（1）Architecture 输入层与隐藏层完全连接  隐藏层与输出层完全连接   node  输入层的每个节点：接受输入变">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jiajun-lab.github.io/2025/09/16/PRMLS/image-20250916100540368.png">
<meta property="og:image" content="https://jiajun-lab.github.io/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250918093217383.png">
<meta property="og:image" content="https://jiajun-lab.github.io/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250918093515315.png">
<meta property="og:image" content="https://jiajun-lab.github.io/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250917143219909.png">
<meta property="og:image" content="https://jiajun-lab.github.io/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250917143538876.png">
<meta property="og:image" content="https://jiajun-lab.github.io/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250917143652316.png">
<meta property="article:published_time" content="2025-09-16T01:42:04.000Z">
<meta property="article:modified_time" content="2025-09-18T03:04:28.241Z">
<meta property="article:author" content="Lee">
<meta property="article:tag" content="ISS">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://jiajun-lab.github.io/2025/09/16/PRMLS/image-20250916100540368.png">
  
  
  
  <title>PRMLS - Lee&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jiajun-lab.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":"tUYpg6nx1J5VCJamrfWcKQkl-gzGzoHsz","app_key":"HM1AWutkOYJ9mXYNjwqc7gSy","server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Lee&#39;s blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="PRMLS"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-09-16 09:42" pubdate>
          2025年9月16日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.6k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          31 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">PRMLS</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="PATTERN-RECOGNITION-AND-MACHINE-LEARNING-SYSTEMS"><a href="#PATTERN-RECOGNITION-AND-MACHINE-LEARNING-SYSTEMS" class="headerlink" title="PATTERN RECOGNITION AND MACHINE LEARNING SYSTEMS"></a>PATTERN RECOGNITION AND MACHINE LEARNING SYSTEMS</h1><h2 id="Day-1"><a href="#Day-1" class="headerlink" title="Day 1"></a>Day 1</h2><h2 id="Day-2"><a href="#Day-2" class="headerlink" title="Day 2"></a>Day 2</h2><h3 id="2-1-Neural-Network-Models-and-Designs"><a href="#2-1-Neural-Network-Models-and-Designs" class="headerlink" title="2.1 Neural Network Models and Designs"></a>2.1 <strong>Neural Network Models and</strong> Designs</h3><h4 id="2-1-1-径向基函数-Radial-Basis-Function-Networks"><a href="#2-1-1-径向基函数-Radial-Basis-Function-Networks" class="headerlink" title="2.1.1 径向基函数 Radial Basis Function Networks"></a>2.1.1 径向基函数 Radial Basis Function Networks</h4><h4 id="（1）Architecture"><a href="#（1）Architecture" class="headerlink" title="（1）Architecture"></a>（1）<strong>Architecture</strong></h4><ol>
<li><p>输入层与隐藏层完全连接</p>
</li>
<li><p>隐藏层与输出层完全连接</p>
</li>
</ol>
<p><strong>node</strong></p>
<ol>
<li>输入层的每个节点：接受输入变量的值</li>
<li>每个隐藏节点：提供变量的径向基函数</li>
<li>输出层的每个节点：对应于输入变量的非线形函数</li>
</ol>
<p><strong>weight</strong></p>
<ol>
<li>和第 i 个内部节点之间的连接上有一个可调权重向量 W，表示径向基函数的中心（通过无监督学习</li>
<li>连接内部节点和输出节点的可调权重（通过监督学习）</li>
</ol>
<h4 id="（2）Active"><a href="#（2）Active" class="headerlink" title="（2）Active"></a>（2）Active</h4><p>第 j 个基函数（核）对位于其核中心 cj 相同径向距离内的所有输入给出相同的激活值。核节点通常使用高斯激活基函数，该函数取决于输入向量 x 和节点中心向量 ci 之间的距离。在这种情况下，隐藏层第 i 个节点的激活值计算如下，其中 σi 是平滑参数（也称为归一化因子）：</p>
<img src="image-20250916100540368.png" srcset="/img/loading.gif" lazyload alt="image-20250916100540368" style="zoom:50%;" />

<h4 id="（3）Training"><a href="#（3）Training" class="headerlink" title="（3）Training"></a>（3）Training</h4><ul>
<li>two parameters (c i and σi ) to be found for each kernel node i</li>
<li>a full weight set for output nodes</li>
</ul>
<p><strong>two stage training</strong></p>
<ul>
<li><p>寻找隐藏层节点的中心和平滑参数值（无监督学习）</p>
<ul>
<li><p>要找到每个 RBF 节点的“中心” (c_i) 和“半径&#x2F;宽度” (\sigma_i)。</p>
</li>
<li><p>这一步通常通过 <strong>聚类算法</strong> 来完成，比如 <strong>C-Means clustering</strong>（常用的是 Hard K-Means）。</p>
</li>
<li><p>简单理解：先把训练样本分成若干簇，每个簇的中心作为一个 RBF 节点的中心，簇的分布决定宽度。</p>
</li>
<li><p><strong>Learning for Kernel Centre (求核中心)</strong></p>
<ul>
<li><p>如果样本数很小，可以直接把每个样本作为一个中心（自增长网络的思想）。</p>
</li>
<li><p>如果样本数很大，先用 K-Means 聚类，把聚类中心作为 RBF 的核中心。</p>
</li>
<li><p>数学表达式：<br>$$<br>c_j &#x3D; \frac{1}{m_j} \sum_{x_i \in \Theta_j} x_i<br>$$</p>
</li>
<li><p>意思是：第 j 个簇的中心 &#x3D; 簇中所有样本的平均值。</p>
</li>
</ul>
</li>
<li><p><strong>Learning for Smoothing Parameter (求平滑参数 σ)</strong></p>
<ul>
<li><p>\sigma_j 决定 RBF 的“宽度”，控制函数对输入的响应范围。</p>
</li>
<li><p>通常由簇内样本到中心的平均距离来决定。</p>
</li>
<li><p>数学表达式：</p>
</li>
<li><p>$$<br>\sigma_j^2 &#x3D; \frac{1}{m_j} \sum_{x \in \Theta_j} (x - c_j)^T(x - c_j)<br>$$</p>
</li>
<li><p>意思是：\sigma_j^2 就是簇内样本与中心的平均平方距离（类似方差）。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>寻找输出节点的权重（监督学习）</p>
<ul>
<li><p>在隐藏层节点确定之后，每个输入样本就能映射成一组 RBF 激活值。</p>
</li>
<li><p>这时，输出层就是一个 <strong>线性模型</strong>：输出 &#x3D; 激活值 × 权重。</p>
</li>
<li><p>可以用 <strong>监督学习方法</strong>（如最小二乘法）来训练这些权重，使输出尽量匹配目标值。</p>
</li>
<li><p><strong>权重更新规则</strong><br>$$<br>W_{ji}(t+1) &#x3D; W_{ji}(t) + \Delta W_{ji}<br>$$</p>
<p>意思：在第 t 次迭代后，权重会更新一个增量。</p>
</li>
<li><p><strong>权重增量计算</strong><br>$$<br>\Delta W_{ji} &#x3D; \eta \delta_j O_i<br>$$</p>
<ul>
<li>\eta：学习率</li>
<li>O_i：隐藏层第 i 个节点的输出（RBF 激活值）</li>
<li>\delta_j：输出层第 j 个节点的误差</li>
</ul>
</li>
<li><p><strong>误差计算</strong></p>
<p>\delta_j &#x3D; T_j - O_j</p>
<ul>
<li>T_j：目标输出（训练样本的标签）</li>
<li>O_j：当前网络在输出层 j 的预测值</li>
</ul>
</li>
<li><p><strong>训练过程</strong></p>
<ul>
<li>不断迭代更新权重</li>
<li>直到 <strong>收敛</strong>（即输出误差足够小或达到最大迭代次数）</li>
</ul>
</li>
<li><p>示例</p>
<ul>
<li><p>输出层权重：</p>
<p>W_{o1} &#x3D; W_{o2} &#x3D; W_{o3} &#x3D; W_{o4} &#x3D; 0.5</p>
</li>
<li><p>学习率：\eta &#x3D; 0.5</p>
</li>
</ul>
<table>
<thead>
<tr>
<th><strong>样本</strong></th>
<th><strong>输入 (In1, In2)</strong></th>
<th><strong>目标输出 (T)</strong></th>
<th><strong>激活的中心</strong></th>
</tr>
</thead>
<tbody><tr>
<td>P1</td>
<td>(0,0)</td>
<td>0</td>
<td>C1</td>
</tr>
<tr>
<td>P2</td>
<td>(0,1)</td>
<td>1</td>
<td>C2</td>
</tr>
<tr>
<td>P3</td>
<td>(1,0)</td>
<td>1</td>
<td>C3</td>
</tr>
<tr>
<td>P4</td>
<td>(1,1)</td>
<td>0</td>
<td>C4</td>
</tr>
</tbody></table>
<p><strong>样本 P1: (0,0), T&#x3D;0</strong></p>
<ul>
<li><p>激活 C1，当前输出 Out&#x3D;0.5</p>
</li>
<li><p>误差：T - Out &#x3D; 0 - 0.5 &#x3D; -0.5</p>
</li>
<li><p>更新：</p>
<p>W_{o1}(t+1) &#x3D; 0.5 + 0.5 \times (-0.5) \times 1 &#x3D; 0.25</p>
</li>
</ul>
<p>结果：W_{o1}&#x3D;0.25, W_{o2}&#x3D;0.5, W_{o3}&#x3D;0.5, W_{o4}&#x3D;0.5</p>
<p><strong>样本 P2: (0,1), T&#x3D;1</strong></p>
<ul>
<li><p>激活 C2，当前输出 Out&#x3D;0.5</p>
</li>
<li><p>误差：1 - 0.5 &#x3D; 0.5</p>
</li>
<li><p>更新：</p>
<p>W_{o2}(t+1) &#x3D; 0.5 + 0.5 \times (0.5) \times 1 &#x3D; 0.75</p>
</li>
</ul>
<p>结果：W_{o1}&#x3D;0.25, W_{o2}&#x3D;0.75, W_{o3}&#x3D;0.5, W_{o4}&#x3D;0.5</p>
<p><strong>样本 P3: (1,0), T&#x3D;1</strong></p>
<ul>
<li><p>激活 C3，当前输出 Out&#x3D;0.5</p>
</li>
<li><p>误差：1 - 0.5 &#x3D; 0.5</p>
</li>
<li><p>更新：</p>
<p>W_{o3}(t+1) &#x3D; 0.5 + 0.5 \times (0.5) \times 1 &#x3D; 0.75</p>
</li>
</ul>
<p>结果：W_{o1}&#x3D;0.25, W_{o2}&#x3D;0.75, W_{o3}&#x3D;0.75, W_{o4}&#x3D;0.5</p>
<p><strong>样本 P4: (1,1), T&#x3D;0</strong></p>
<ul>
<li><p>激活 C4，当前输出 Out&#x3D;0.5</p>
</li>
<li><p>误差：0 - 0.5 &#x3D; -0.5</p>
</li>
<li><p>更新：</p>
<p>W_{o4}(t+1) &#x3D; 0.5 + 0.5 \times (-0.5) \times 1 &#x3D; 0.25</p>
</li>
</ul>
<p>结果：W_{o1}&#x3D;0.25, W_{o2}&#x3D;0.75, W_{o3}&#x3D;0.75, W_{o4}&#x3D;0.25</p>
<p><strong>最终权重表</strong></p>
<table>
<thead>
<tr>
<th><strong>权重</strong></th>
<th><strong>初始值</strong></th>
<th><strong>P1后</strong></th>
<th><strong>P2后</strong></th>
<th><strong>P3后</strong></th>
<th><strong>P4后</strong></th>
</tr>
</thead>
<tbody><tr>
<td>W_o1</td>
<td>0.5</td>
<td>0.25</td>
<td>0.25</td>
<td>0.25</td>
<td>0.25</td>
</tr>
<tr>
<td>W_o2</td>
<td>0.5</td>
<td>0.5</td>
<td>0.75</td>
<td>0.75</td>
<td>0.75</td>
</tr>
<tr>
<td>W_o3</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.75</td>
<td>0.75</td>
</tr>
<tr>
<td>W_o4</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.25</td>
</tr>
</tbody></table>
<ul>
<li>对应 <strong>输出应为 0 的模式 (P1, P4)</strong> → 权重下降 (0.25)</li>
<li>对应 <strong>输出应为 1 的模式 (P2, P3)</strong> → 权重上升 (0.75)</li>
<li>最终网络能正确区分 XOR：<ul>
<li>(0,0) → 0</li>
<li>(0,1) → 1</li>
<li>(1,0) → 1</li>
<li>(1,1) → 0</li>
</ul>
</li>
</ul>
<p>RBF 学习过程中，<strong>每个训练样本只更新一个对应的核权重</strong>，经过几次迭代后，权重分化为 {低 (0.25), 高 (0.75)}，成功学会 XOR 逻辑。</p>
</li>
</ul>
</li>
</ul>
<h4 id="2-1-2-General-Regression-Neural-Networks-GRNN"><a href="#2-1-2-General-Regression-Neural-Networks-GRNN" class="headerlink" title="2.1.2 General Regression Neural Networks (GRNN)"></a>2.1.2 General Regression Neural Networks (GRNN)</h4><h4 id="（1）Architecture-1"><a href="#（1）Architecture-1" class="headerlink" title="（1）Architecture"></a>（1）<strong>Architecture</strong></h4><ul>
<li>an input-layer</li>
<li>three computational layers: pattern, summation, output</li>
</ul>
<p><strong>Nodes &amp; Connections</strong></p>
<ul>
<li>输入层：将输入模式传递到模式层 (pattern layer)，通过全连接权重。</li>
<li>模式层 (Pattern layer)：每个模式节点计算输入向量与权值向量的距离。</li>
<li>求和层 (Summation layer)：分为 <strong>A 型节点</strong>和 <strong>B 型节点</strong>，对模式层输出做加权求和。</li>
<li>输出层：对 A、B 两类求和节点的结果做比值运算，得到回归输出。</li>
</ul>
<p><strong>Activation</strong></p>
<ul>
<li><p>模式层计算距离：</p>
</li>
<li><p>$$<br>D_i^2 &#x3D; (x - w_i)^T (x - w_i)<br>$$</p>
</li>
<li><p>模式层激活函数（常用高斯核）：</p>
</li>
<li><p>$$<br>O_i &#x3D; \exp \left(-\frac{D_i^2}{2\sigma^2}\right)<br>$$</p>
</li>
<li><p>输出层回归公式：</p>
</li>
<li><p>$$<br>\hat{z} &#x3D; \frac{\sum_{i&#x3D;1}^P A_i \exp(-D_i^2&#x2F;2\sigma^2)}{\sum_{i&#x3D;1}^P B_i \exp(-D_i^2&#x2F;2\sigma^2)}<br>$$</p>
</li>
<li><ul>
<li>P：模式节点数</li>
<li>\sigma：平滑参数</li>
</ul>
</li>
</ul>
<p><strong>Learning of GRNN</strong></p>
<ul>
<li><p>模式层：每个训练样本对应一个新节点，权值设为样本或簇中心。</p>
</li>
<li><p>求和层：每次看到属于簇 i 的样本 z_j，更新：</p>
<p>A_i(k) &#x3D; A_i(k-1) + z_j, \quad B_i(k) &#x3D; B_i(k-1) + 1</p>
<p>初始 A_i(0)&#x3D;0, B_i(0)&#x3D;0。</p>
</li>
<li><p>平滑常数 \sigma：实验确定，控制决策边界。</p>
</li>
</ul>
<p><strong>Mapping Surface &amp; σ)</strong></p>
<ul>
<li>小 \sigma：表面尖锐，拟合点附近很好，但泛化差。</li>
<li>大 \sigma：表面平滑，泛化更好，但过度平滑可能欠拟合。</li>
<li><strong>好泛化效果需要在两者之间权衡</strong>。</li>
</ul>
<p><strong>Summary</strong></p>
<p><strong>特点</strong></p>
<ul>
<li>基于记忆 (memory-based)，不丢弃训练样本。</li>
<li>一次学习算法 (one-pass learning)，无需反复迭代。</li>
<li>自增长网络：每个新样本生成一个新模式节点。</li>
</ul>
<p><strong>应用</strong></p>
<ul>
<li>适合 <strong>预测、控制、建模</strong>等任务，尤其在训练数据分布未知时。</li>
<li>优点：<ul>
<li>随着样本的增加，估计值会收敛到真实的回归曲面。</li>
<li>广义相对论神经网络 (GRNN) 能够处理稀疏数据和实时环境，因为回归曲面在任何地方都能即时定义，并且网络能够实现从一个观测值到另一个观测值的平滑过渡。</li>
<li>训练 GRNN 快速而简单，只需对训练集进行一次遍历。网络在提供单个训练样本后即可开始执行回归。</li>
</ul>
</li>
<li>缺点：<ul>
<li>由于训练集中的每个模式都会添加一个模式单元，因此网络可能会变得非常庞大（除非使用聚类来确定原型中心）。</li>
<li>网络的计算负载相对较重。</li>
</ul>
</li>
</ul>
<p>GRNN 是一种基于 RBF 的记忆型神经网络，通过模式层的高斯核映射 + 求和层比值计算，实现快速的一次性学习，特别适合回归和预测问题。</p>
<h4 id="2-1-3-Self-Organizing-Map-SOM-Kohonen-Network-学习笔记"><a href="#2-1-3-Self-Organizing-Map-SOM-Kohonen-Network-学习笔记" class="headerlink" title="2.1.3 Self-Organizing Map (SOM, Kohonen Network) 学习笔记"></a>2.1.3 Self-Organizing Map (SOM, Kohonen Network) 学习笔记</h4><p><strong>1. 基本概念</strong></p>
<ul>
<li><strong>SOM 是一种用于聚类与降维的神经网络</strong></li>
<li>特点：<strong>无监督学习</strong>，通过竞争学习 (winner-takes-all) 将高维输入映射到低维空间 (通常是 1D 或 2D 网格)</li>
<li>网络结构：<ul>
<li><strong>输入层</strong>：接收输入向量</li>
<li>**输出层 (Cluster Layer)**：一组神经元，通常排列成一维或二维网格，每个神经元代表一个“簇”</li>
</ul>
</li>
</ul>
<p><strong>2. 竞争学习机制 (Competitive Learning)</strong></p>
<ul>
<li><p>输入模式与所有输出神经元的权重比较 → 找到距离最近的神经元 (Best Matching Unit, BMU)</p>
</li>
<li><p>胜者神经元及其邻居调整权重，向输入向量靠近</p>
</li>
<li><p>数学条件：</p>
</li>
<li><p>$$<br>|| w_i - x || \leq || w_k - x ||, \quad \forall k \neq i<br>$$</p>
</li>
<li><p>即选择与输入最接近的神经元作为赢家</p>
</li>
</ul>
<p><strong>3. Kohonen 学习规则</strong></p>
<ul>
<li><p>更新权重：</p>
</li>
<li><p>$$<br>w_i(t+1) &#x3D; w_i(t) + \eta(t) \cdot h_{ci}(t) \cdot (x(t) - w_i(t))<br>$$</p>
<ul>
<li>\eta(t)：学习率，随时间递减</li>
<li>h_{ci}(t)：邻域函数，控制 BMU 周围神经元的更新幅度</li>
</ul>
</li>
<li><p>胜者更新最大，邻居也会部分更新，形成空间拓扑映射</p>
</li>
</ul>
<p><strong>4. SOM 的结构形式</strong></p>
<ul>
<li><strong>一维 SOM</strong>：输出层为一条直线，神经元顺序排列</li>
<li><strong>二维 SOM</strong>：输出层为一个网格 (lattice)，适合可视化高维数据</li>
<li>拓扑结构保证了相似输入在映射空间中也会相邻</li>
</ul>
<p><strong>5. 特点与应用</strong></p>
<p><strong>特点</strong></p>
<ul>
<li>无监督聚类</li>
<li>保持拓扑关系（相似输入 → 相邻输出）</li>
<li>降维可视化（高维数据映射到 2D 网格）</li>
</ul>
<p><strong>应用</strong></p>
<ul>
<li><p>数据聚类与模式识别</p>
</li>
<li><p>高维数据可视化 (如文本、基因数据)</p>
</li>
<li><p>图像分析、语音识别</p>
</li>
<li><p>客户细分、市场分析</p>
</li>
<li><p>SOM 通过 <strong>竞争学习 + 邻域更新</strong>，能把复杂高维数据映射到低维拓扑结构中。</p>
</li>
<li><p><strong>Winner-takes-all</strong> 确保最佳匹配单元学习输入特征，邻居共享更新，形成连续的“特征地图”。</p>
</li>
<li><p>它既能做 <strong>聚类</strong>，又能做 <strong>可视化与降维</strong>。</p>
</li>
</ul>
<p><strong>6. Kohonen 学习算法步骤</strong></p>
<ol>
<li><p><strong>初始化权重</strong>：所有权重向量 w_r 用均匀分布随机初始化 (0,1)。</p>
</li>
<li><p><strong>输入信号</strong>：将输入向量 x 输入网络。</p>
</li>
<li><p>**选择获胜单元 (BMU)**：找到与输入最接近的权重向量</p>
</li>
<li><p>$$<br>||x - w_r|| &#x3D; \min_r ||x - w_r||<br>$$</p>
</li>
<li><p><strong>更新权重</strong></p>
<ul>
<li><p>$$<br>w_r^{new} &#x3D; w_r^{old} + \alpha \cdot h_{rr’} \cdot (x - w_r^{old})<br>$$</p>
</li>
<li><p>邻域神经元也更新，幅度随距离衰减。</p>
</li>
<li><p>其中：\alpha 为学习率，h_{rr’} 为邻域函数。</p>
</li>
</ul>
</li>
<li><p><strong>重复步骤 2–4</strong>，直到网络权重收敛。</p>
</li>
<li><p><strong>逐步减小学习率与邻域范围</strong>，提升收敛稳定性。</p>
</li>
</ol>
<p><strong>7. 邻域函数 (Neighbourhood Function)</strong></p>
<ul>
<li>决定 BMU 周围哪些神经元会被更新，以及更新幅度。</li>
<li>常见形式：<ul>
<li><strong>方形邻域</strong></li>
<li><strong>六边形邻域</strong></li>
<li>**高斯函数 (Gaussian)**：平滑衰减效果更自然。</li>
</ul>
</li>
</ul>
<p><strong>8. SOM 映射过程</strong></p>
<ul>
<li>输入模式空间被映射到神经网络格子 (lattice)。</li>
<li>BMU 及其邻居的权重向输入靠近：<ul>
<li><strong>越靠近 BMU → 更新越大</strong></li>
<li><strong>距离远的 → 更新越小</strong></li>
</ul>
</li>
<li>结果：输入数据的拓扑结构在 SOM 网格上得到保留。</li>
</ul>
<p><strong>9. SOM 仿真例子</strong></p>
<ul>
<li>将二维输入 (x_1, x_2) 映射到 SOM 网格<ul>
<li>(a) 输入数据：在 [0,1] \times [0,1] 范围内均匀分布</li>
<li>(b) 初始权重：随机分布</li>
<li>(c) 迭代 30 次后，权重逐渐向数据分布靠近</li>
<li>(d) 迭代 1000 次后，SOM 权重形成平滑网格，保持拓扑关系</li>
</ul>
</li>
</ul>
<p><strong>10. SOM 的优点</strong></p>
<ul>
<li>不需要事先指定簇数（克服了 C-Means 的局限）。</li>
<li>能同时进行 <strong>聚类</strong> 和 <strong>降维</strong>。</li>
<li>提供了一种 <strong>数据压缩</strong> 方法。</li>
<li>对有标签数据，还能作为分类器。</li>
<li>最重要的是：提供 <strong>数据可视化</strong> 功能，可直观展现高维数据结构。</li>
</ul>
<h3 id="2-2-Deep-neural-networks-and-deep-learning-systems"><a href="#2-2-Deep-neural-networks-and-deep-learning-systems" class="headerlink" title="2.2 Deep neural networks and deep learning systems"></a>2.2 <strong>Deep neural networks and deep learning systems</strong></h3><h4 id="2-2-1-Filtering"><a href="#2-2-1-Filtering" class="headerlink" title="2.2.1 Filtering"></a>2.2.1 <strong>Filtering</strong></h4><img src="/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250918093217383.png" srcset="/img/loading.gif" lazyload alt="image-20250918093217383" style="zoom:33%;" />

<h4 id="2-2-2-Features-classifier"><a href="#2-2-2-Features-classifier" class="headerlink" title="2.2.2 Features + classifier"></a>2.2.2 <strong>Features + classifier</strong></h4><img src="/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250918093515315.png" srcset="/img/loading.gif" lazyload alt="image-20250918093515315" style="zoom:50%;" />

<h4 id="2-2-3-Learning-the-features"><a href="#2-2-3-Learning-the-features" class="headerlink" title="2.2.3 Learning the features"></a>2.2.3 <strong>Learning the features</strong></h4><h4 id="2-2-4-Comparison"><a href="#2-2-4-Comparison" class="headerlink" title="2.2.4 Comparison"></a>2.2.4 <strong>Comparison</strong></h4><table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th><strong>Machine Learning</strong></th>
<th><strong>Deep Learning</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Data dependencies</strong></td>
<td>Excellent performances on a small&#x2F;medium dataset</td>
<td>Excellent performance on a big dataset</td>
</tr>
<tr>
<td><strong>Hardware dependencies</strong></td>
<td>Work on a low-end machine</td>
<td>Requires powerful machine</td>
</tr>
<tr>
<td><strong>Feature engineering</strong></td>
<td>Need to understand the features that represent the data</td>
<td>No need to understand the learned features</td>
</tr>
<tr>
<td><strong>Execution time</strong></td>
<td>From few minutes to hours</td>
<td>Up to weeks</td>
</tr>
<tr>
<td><strong>Interpretability</strong></td>
<td>Possible for some (logistic regression, decision tree); some not possible (SVM, XGBoost)</td>
<td>Difficult to impossible</td>
</tr>
</tbody></table>
<h4 id="2-2-5-Components-in-deep-learning"><a href="#2-2-5-Components-in-deep-learning" class="headerlink" title="2.2.5 Components in deep learning"></a>2.2.5 <strong>Components in deep learning</strong></h4><p><strong>2D convolution  &amp; with padding</strong></p>
<p>When the stride is (1, 1)</p>
<p>(<strong>stride along row direction</strong>, <strong>stride along column direction</strong>)</p>
<ul>
<li><p>行方向（Row）输出大小：<br>$$<br>M_r &#x3D; \left\lfloor \frac{W_r + P_r - F_r}{S_r} \right\rfloor + 1<br>$$</p>
</li>
<li><p>列方向（Column）输出大小：<br>$$<br>M_c &#x3D; \left\lfloor \frac{W_c + P_c - F_c}{S_c} \right\rfloor + 1<br>$$</p>
</li>
<li></li>
</ul>
<p><strong>参数数量计算</strong></p>
<ul>
<li><strong>输入张量大小</strong>：(I_r, I_c, C_i)<ul>
<li>I_r：输入行数 (height)</li>
<li>I_c：输入列数 (width)</li>
<li>C_i：输入通道数 (channels)，比如 RGB 图像就是 3</li>
</ul>
</li>
<li><strong>卷积核大小</strong>：(F_r, F_c)<ul>
<li>F_r：卷积核行数 (kernel height)</li>
<li>F_c：卷积核列数 (kernel width)</li>
</ul>
</li>
<li>**输出特征图 (feature maps)**：D 个</li>
</ul>
<p>这就是公式：<br>$$<br>p_{tr} &#x3D; [C_i \times (F_r \times F_c) + 1] \times D<br>$$<br>假设：</p>
<ul>
<li>输入：(32, 32, 3)</li>
<li>卷积核大小：(3, 3)</li>
<li>输出 feature maps 数：D &#x3D; 32</li>
</ul>
<p>计算参数数：<br>$$<br>p_{tr} &#x3D; [3 \times (3 \times 3) + 1] \times 32</p>
<p>&#x3D; [3 \times 9 + 1] \times 32</p>
<p>&#x3D; [28] \times 32 &#x3D; 896<br>$$<br>这正好就是 <strong>Conv2D(filters&#x3D;32, kernel_size&#x3D;(3,3), input_channels&#x3D;3)</strong> 的参数数。</p>
<p>卷积层的参数 &#x3D; <strong>卷积核大小 × 输入通道数 + bias</strong>，再 × 输出特征图个数。</p>
<p><strong>参数说明</strong></p>
<ul>
<li><strong>Mr, Mc</strong>: 输出尺寸（行、列）</li>
<li><strong>Wr, Wc</strong>: 输入尺寸（行、列）</li>
<li><strong>Fr, Fc</strong>: 卷积核（Filter）大小（行、列）</li>
<li><strong>Pr, Pc</strong>: 填充（Padding）大小（行、列）</li>
<li><strong>Sr, Sc</strong>: 步幅（Stride）大小（行、列）</li>
</ul>
<p><strong>Maxpooling</strong></p>
<ul>
<li><p>行方向（Row）：<br>$$<br>M_r &#x3D; \left\lfloor \frac{W_r - F_r}{S_r} \right\rfloor + 1<br>$$</p>
</li>
<li><p>列方向（Column）：<br>$$<br>M_c &#x3D; \left\lfloor \frac{W_c - F_c}{S_c} \right\rfloor + 1<br>$$</p>
</li>
<li><p><strong>Mr, Mc</strong>: 输出尺寸（行、列）</p>
</li>
<li><p><strong>Wr, Wc</strong>: 输入尺寸（行、列）</p>
</li>
<li><p><strong>Fr, Fc</strong>: 池化核（Filter）大小（行、列）</p>
</li>
<li><p><strong>Sr, Sc</strong>: 步幅（Stride）大小（行、列）</p>
</li>
</ul>
<h2 id="Day-3"><a href="#Day-3" class="headerlink" title="Day 3"></a>Day 3</h2><h3 id="3-1-UAT"><a href="#3-1-UAT" class="headerlink" title="3.1 UAT"></a>3.1 UAT</h3><p><strong>1. 理论</strong></p>
<p><strong>理论上存在Universal Approximation Theorem，可以在一层网络中实现好的分类效果。</strong></p>
<ul>
<li>单层前馈网络（只要隐藏层神经元足够多） → <strong>理论上可以逼近任意函数</strong>。</li>
<li><strong>神经网络的潜在表达能力</strong>。</li>
</ul>
<p><strong>2. 局限性</strong></p>
<ul>
<li><strong>没有学习算法能在实践中有效训练这样一个超大单层网络</strong>。</li>
<li>定理只告诉我们“存在性”，但 <strong>没有提供如何找到合适参数的方法</strong>。</li>
<li>单层网络在实践中会遇到：<ul>
<li>参数量爆炸</li>
<li>训练效率低</li>
<li>泛化能力差</li>
</ul>
</li>
</ul>
<p><strong>3. 实践经验</strong></p>
<ul>
<li>过去十多年研究表明：<ul>
<li><strong>网络深度</strong> 是性能提升的关键。</li>
<li>深层网络比单层网络更高效，能用更少的参数学习更复杂的模式。</li>
<li>深度带来了 **层级特征表示 (hierarchical representation)**，这是现代深度学习成功的核心。</li>
</ul>
</li>
</ul>
<p>✅ 一句话总结：</p>
<p><strong>UAT 证明了单层网络“能做到”，但深度学习告诉我们“如何高效做到”。</strong></p>
<h3 id="3-2-Importance-of-Depth-Multiple-inputs-outputs"><a href="#3-2-Importance-of-Depth-Multiple-inputs-outputs" class="headerlink" title="3.2 Importance of Depth &amp; Multiple inputs &amp; outputs"></a>3.2 Importance of Depth &amp; Multiple inputs &amp; outputs</h3><p>Depth is the key</p>
<p><strong>Multiple Inputs</strong></p>
<p><img src="/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250917143219909.png" srcset="/img/loading.gif" lazyload alt="image-20250917143219909"></p>
<p><strong>Multiple Outputs</strong></p>
<p><img src="/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250917143538876.png" srcset="/img/loading.gif" lazyload alt="image-20250917143538876"></p>
<p><strong>Shared Layers</strong></p>
<p><img src="/Users/lijiajun/Documents/code/hexo_myblog/source/_posts/PRMLS/image-20250917143652316.png" srcset="/img/loading.gif" lazyload alt="image-20250917143652316"></p>
<h3 id="3-3-Recurrent-neural-network"><a href="#3-3-Recurrent-neural-network" class="headerlink" title="3.3 Recurrent neural network"></a>3.3 Recurrent neural network</h3><p>循环神经网络</p>
<ul>
<li>循环神经网络：对序列中的元素&#x2F;片段执行相同计算的网络</li>
<li>当前元素&#x2F;片段的输出取决于先前元素&#x2F;片段的输出</li>
</ul>
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>CNN (Convolutional Neural Network)</strong></th>
<th><strong>RNN (Recurrent Neural Network &#x2F; LSTM)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Usage 应用场景</strong></td>
<td>适合空间数据，如图像、视频</td>
<td>适合时序&#x2F;序列数据，如文本、语音</td>
</tr>
<tr>
<td><strong>Capability 能力</strong></td>
<td>比 RNN 更强大，计算更快</td>
<td>相对较弱，计算较慢</td>
</tr>
<tr>
<td><strong>Input 输入特性</strong></td>
<td>固定大小输入，固定大小输出</td>
<td>可处理任意长度输入&#x2F;输出</td>
</tr>
<tr>
<td><strong>Nature 本质</strong></td>
<td>局部连接 + 权重共享（卷积操作）</td>
<td>循环连接，利用时间依赖关系</td>
</tr>
</tbody></table>
<h2 id="Day-4"><a href="#Day-4" class="headerlink" title="Day 4"></a>Day 4</h2><p><strong>Case studies on using convolutional neural networks for machine learning systems</strong></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/class-note/" class="category-chain-item">class_note</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/ISS/" class="print-no-link">#ISS</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>PRMLS</div>
      <div>https://jiajun-lab.github.io/2025/09/16/PRMLS/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Lee</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年9月16日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/09/18/CS50IAP/" title="CS50IAP">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">CS50IAP</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/09/16/%E7%AC%94%E8%AF%95%E7%9C%9F%E9%A2%98/" title="">
                        <span class="hidden-mobile"></span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"tUYpg6nx1J5VCJamrfWcKQkl-gzGzoHsz","appKey":"HM1AWutkOYJ9mXYNjwqc7gSy","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  

</div>

  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
